Index: tests/test_sanitizer.py
===================================================================
--- tests/test_sanitizer.py	(revision 1093)
+++ tests/test_sanitizer.py	(working copy)
@@ -1,26 +1,42 @@
 import os,sys,unittest
 from support import simplejson, html5lib_test_files
 
-from html5lib import html5parser, sanitizer, constants
+from html5lib import html5parser, sanitizer, constants, treebuilders, treewalkers, serializer
 
 class SanitizeTest(unittest.TestCase):
-  def addTest(cls, name, expected, input):
+  def addTest(cls, name, expected, input, strip=False):
     def test(self, expected=expected, input=input):
-        expected = ''.join([token.toxml() for token in html5parser.HTMLParser().
-          parseFragment(expected).childNodes])
         expected = simplejson.loads(simplejson.dumps(expected))
-        self.assertEqual(expected, self.sanitize_html(input))
+        self.assertEqual(expected, self.sanitize_html(input, strip=strip))
     setattr(cls, name, test)
   addTest = classmethod(addTest)
 
-  def sanitize_html(self,stream):
+  def sanitize_html2(self,stream):
     return ''.join([token.toxml() for token in
        html5parser.HTMLParser(tokenizer=sanitizer.HTMLSanitizer).
            parseFragment(stream).childNodes])
 
+  def sanitize_html(self, data, encoding=None, strip=False):
+    
+    def sanitizer_factory(*args, **kwargs):
+      san = sanitizer.HTMLSanitizer(*args, **kwargs)
+      san.strip_tokens = strip
+      return san
+
+    parser = html5parser.HTMLParser(tree=treebuilders.getTreeBuilder("dom"),
+        tokenizer=sanitizer_factory)
+    walker = treewalkers.getTreeWalker("dom")
+    stream = walker(parser.parseFragment(data, encoding=encoding))
+    slzr = serializer.htmlserializer.HTMLSerializer(omit_optional_tags=False,
+        quote_attr_values=True, 
+        use_trailing_solidus=True, space_before_trailing_solidus=False)
+    return slzr.render(stream, encoding)
+
   def test_should_handle_astral_plane_characters(self):
     self.assertEqual(u"<p>\U0001d4b5 \U0001d538</p>",
       self.sanitize_html("<p>&#x1d4b5; &#x1d538;</p>"))
+    self.assertEqual(u"<p>\U0001d4b5 \U0001d538</p>",
+      self.sanitize_html("<p>&#x1d4b5; &#x1d538;</p>", strip=True))
 
 for tag_name in sanitizer.HTMLSanitizer.allowed_elements:
     if tag_name in ['caption', 'col', 'colgroup', 'optgroup', 'option', 'table', 'tbody', 'td', 'tfoot', 'th', 'thead', 'tr']: continue ### TODO
@@ -29,24 +45,49 @@
         SanitizeTest.addTest("test_should_allow_%s_tag" % tag_name,
           "<img title=\"1\"/>foo &lt;bad&gt;bar&lt;/bad&gt; baz",
           "<%s title='1'>foo <bad>bar</bad> baz</%s>" % (tag_name,tag_name))
+        SanitizeTest.addTest("test_should_allow_%s_tag" % tag_name,
+          "<img title=\"1\"/>foo bar baz",
+          "<%s title='1'>foo <bad>bar</bad> baz</%s>" % (tag_name,tag_name), 
+          strip=True)
     elif tag_name == 'br':
         SanitizeTest.addTest("test_should_allow_%s_tag" % tag_name,
           "<br title=\"1\"/>foo &lt;bad&gt;bar&lt;/bad&gt; baz<br/>",
           "<%s title='1'>foo <bad>bar</bad> baz</%s>" % (tag_name,tag_name))
+        SanitizeTest.addTest("test_should_allow_%s_tag" % tag_name,
+          "<br title=\"1\"/>foo bar baz<br/>",
+          "<%s title='1'>foo <bad>bar</bad> baz</%s>" % (tag_name,tag_name),
+          strip=True)
     elif tag_name in constants.voidElements:
         SanitizeTest.addTest("test_should_allow_%s_tag" % tag_name,
           "<%s title=\"1\"/>foo &lt;bad&gt;bar&lt;/bad&gt; baz" % tag_name,
           "<%s title='1'>foo <bad>bar</bad> baz</%s>" % (tag_name,tag_name))
+        SanitizeTest.addTest("test_should_allow_%s_tag" % tag_name,
+          "<%s title=\"1\"/>foo bar baz" % tag_name,
+          "<%s title='1'>foo <bad>bar</bad> baz</%s>" % (tag_name,tag_name),
+          strip=True)
     else:
         SanitizeTest.addTest("test_should_allow_%s_tag" % tag_name,
           "<%s title=\"1\">foo &lt;bad&gt;bar&lt;/bad&gt; baz</%s>" % (tag_name,tag_name),
           "<%s title='1'>foo <bad>bar</bad> baz</%s>" % (tag_name,tag_name))
+        # textarea and title are treated differently
+        if tag_name in ["textarea", "title"]:
+          SanitizeTest.addTest("test_should_allow_%s_tag" % tag_name,
+            "<%s title=\"1\">foo &lt;bad&gt;bar&lt;/bad&gt; baz</%s>" % (tag_name,tag_name),
+            "<%s title='1'>foo <bad>bar</bad> baz</%s>" % (tag_name,tag_name),
+            strip=True)
+        else:
+          SanitizeTest.addTest("test_should_allow_%s_tag" % tag_name,
+            "<%s title=\"1\">foo bar baz</%s>" % (tag_name,tag_name),
+            "<%s title='1'>foo <bad>bar</bad> baz</%s>" % (tag_name,tag_name),
+            strip=True)
 
 for tag_name in sanitizer.HTMLSanitizer.allowed_elements:
     tag_name = tag_name.upper()
     SanitizeTest.addTest("test_should_forbid_%s_tag" % tag_name,
       "&lt;%s title=\"1\"&gt;foo &lt;bad&gt;bar&lt;/bad&gt; baz&lt;/%s&gt;" % (tag_name,tag_name),
       "<%s title='1'>foo <bad>bar</bad> baz</%s>" % (tag_name,tag_name))
+    SanitizeTest.addTest("test_should_forbid_%s_tag" % tag_name, "foo bar baz", 
+      "<%s title='1'>foo <bad>bar</bad> baz</%s>" % (tag_name,tag_name), strip=True)
 
 for attribute_name in sanitizer.HTMLSanitizer.allowed_attributes:
     if attribute_name != attribute_name.lower(): continue ### TODO
@@ -54,35 +95,46 @@
     SanitizeTest.addTest("test_should_allow_%s_attribute" % attribute_name,
       "<p %s=\"foo\">foo &lt;bad&gt;bar&lt;/bad&gt; baz</p>" % attribute_name,
       "<p %s='foo'>foo <bad>bar</bad> baz</p>" % attribute_name)
+    SanitizeTest.addTest("test_should_allow_%s_attribute" % attribute_name,
+      "<p %s=\"foo\">foo bar baz</p>" % attribute_name,
+      "<p %s='foo'>foo <bad>bar</bad> baz</p>" % attribute_name, strip=True)
 
 for attribute_name in sanitizer.HTMLSanitizer.allowed_attributes:
     attribute_name = attribute_name.upper()
     SanitizeTest.addTest("test_should_forbid_%s_attribute" % attribute_name,
       "<p>foo &lt;bad&gt;bar&lt;/bad&gt; baz</p>",
       "<p %s='display: none;'>foo <bad>bar</bad> baz</p>" % attribute_name)
+    SanitizeTest.addTest("test_should_forbid_%s_attribute" % attribute_name,
+      "<p>foo bar baz</p>",
+      "<p %s='display: none;'>foo <bad>bar</bad> baz</p>" % attribute_name,
+      strip=True)
 
 for protocol in sanitizer.HTMLSanitizer.allowed_protocols:
     SanitizeTest.addTest("test_should_allow_%s_uris" % protocol,
       "<a href=\"%s\">foo</a>" % protocol,
       """<a href="%s">foo</a>""" % protocol)
+    SanitizeTest.addTest("test_should_allow_%s_uris" % protocol,
+      "<a href=\"%s\">foo</a>" % protocol,
+      """<a href="%s">foo</a>""" % protocol, strip=True)
 
 for protocol in sanitizer.HTMLSanitizer.allowed_protocols:
     SanitizeTest.addTest("test_should_allow_uppercase_%s_uris" % protocol,
       "<a href=\"%s\">foo</a>" % protocol,
       """<a href="%s">foo</a>""" % protocol)
+    SanitizeTest.addTest("test_should_allow_uppercase_%s_uris" % protocol,
+      "<a href=\"%s\">foo</a>" % protocol,
+      """<a href="%s">foo</a>""" % protocol, strip=True)
 
 def buildTestSuite():
     for filename in html5lib_test_files("sanitizer"):
         for test in simplejson.load(file(filename)):
           SanitizeTest.addTest('test_' + test['name'], test['output'], test['input'])
+        for test in simplejson.load(file(filename)):
+          SanitizeTest.addTest('test_strip_' + test['name'], test['stripped'],
+            test['input'], strip=True)
 
     return unittest.TestLoader().loadTestsFromTestCase(SanitizeTest)
 
-def sanitize_html(stream):
-  return ''.join([token.toxml() for token in
-      html5parser.HTMLParser(tokenizer=sanitizer.HTMLSanitizer).
-          parseFragment(stream).childNodes])
-
 def main():
     buildTestSuite()
     unittest.main()
Index: src/html5lib/sanitizer.py
===================================================================
--- src/html5lib/sanitizer.py	(revision 1093)
+++ src/html5lib/sanitizer.py	(working copy)
@@ -110,6 +110,10 @@
         'mailto', 'news', 'gopher', 'nntp', 'telnet', 'webcal',
         'xmpp', 'callto', 'feed', 'urn', 'aim', 'rsync', 'tag',
         'ssh', 'sftp', 'rtsp', 'afs' ]
+
+    # block elements whose contents will be stripped completely if we we are
+    # stripping tokens during sanitization.
+    unacceptable_block_elements = [ 'script', 'style' ]
   
     # subclasses may define their own versions of these constants
     allowed_elements = acceptable_elements + mathml_elements + svg_elements
@@ -130,7 +134,7 @@
     #    => &lt;script> do_nasty_stuff() &lt;/script>
     #   sanitize_html('<a href="javascript: sucker();">Click here for $100</a>')
     #    => <a>Click here for $100</a>
-    def sanitize_token(self, token):
+    def sanitize_token(self, token, strip_tokens=False):
         if token["type"] in ["StartTag", "EndTag", "EmptyTag"]:
             if token["name"] in self.allowed_elements:
                 if token.has_key("data"):
@@ -145,6 +149,8 @@
                     token["data"] = [[name,val] for name,val in attrs.items()]
                 return token
             else:
+                if strip_tokens:
+                    return None
                 if token["type"] == "EndTag":
                     token["data"] = "</%s>" % token["name"]
                 elif token["data"]:
@@ -188,15 +194,32 @@
         return ' '.join(clean)
 
 class HTMLSanitizer(HTMLTokenizer, HTMLSanitizerMixin):
+    # strip tokens instead of escaping them 
+    strip_tokens = False
+
     def __init__(self, stream, encoding=None, parseMeta=True, useChardet=True,
                  lowercaseElementName=False, lowercaseAttrName=False):
         #Change case matching defaults as we only output lowercase html anyway
         #This solution doesn't seem ideal...
         HTMLTokenizer.__init__(self, stream, encoding, parseMeta, useChardet,
-                               lowercaseElementName, lowercaseAttrName)
+                              lowercaseElementName, lowercaseAttrName)
+        # flag to indicate if stripping is going on or not
+        self.stripping = 0
 
     def __iter__(self):
         for token in HTMLTokenizer.__iter__(self):
-            token = self.sanitize_token(token)
-            if token:
-                yield token
+            # if its a start tag and is a risky block element (e.g. script), we
+            # indicate that we are in striping mode. Its a counter which allows us
+            # to handle nested risky block elements
+            if self.strip_tokens and token["type"] in ["StartTag", "EndTag"] \
+                and token["name"].lower() in HTMLSanitizerMixin.unacceptable_block_elements:
+                if token["type"] == "StartTag":
+                    self.stripping += 1
+                elif token["type"] == "EndTag":
+                    self.stripping -= 1
+
+            # Only yield tokens if we are not in stripping mode
+            if self.stripping < 1:
+                token = self.sanitize_token(token, self.strip_tokens)
+                if token:
+                    yield token
